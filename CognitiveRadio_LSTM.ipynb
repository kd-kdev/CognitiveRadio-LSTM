{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szOWLo_3FYsy"
      },
      "source": [
        "# PyTorch Cognitive Radio - Channel occupancy prediction using LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "gYBPhmELZqmn"
      },
      "source": [
        ">[PyTorch Cognitive Radio - Channel occupancy prediction using LSTM](#scrollTo=szOWLo_3FYsy)\n",
        "\n",
        ">>[1. Import libraries & setup](#scrollTo=R-uOLgZWcB4I)\n",
        "\n",
        ">>[2. Hyperparameters](#scrollTo=8-eDK_jJLcMN)\n",
        "\n",
        ">>[3. Plotting functions](#scrollTo=T-6tgFbULffj)\n",
        "\n",
        ">>[4. Simulate channel data](#scrollTo=gVkPvKYpLuhe)\n",
        "\n",
        ">>[5. Sequence creation & DataLoader](#scrollTo=7-D_fGKNL6Jz)\n",
        "\n",
        ">>[6. LSTM model definition](#scrollTo=NXTUz7rOMFLL)\n",
        "\n",
        ">>[7. Training loop](#scrollTo=RHSHqKGMMUZw)\n",
        "\n",
        ">>[8. Evaluate models](#scrollTo=0AvRdlELMbBy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-uOLgZWcB4I"
      },
      "source": [
        "## 1.&nbsp;Import libraries & setup\n",
        "Imports all related libraries & installs packages (includes optional visualization libs), may require re-run after runtime restart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RQzlOFfpDFmI"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "def ensure(pkg, pip_name=None):\n",
        "    pip_name = pip_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "        print(f\"{pkg} already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pip_name}...\")\n",
        "        !pip install -q {pip_name}\n",
        "\n",
        "ensure(\"torchview\")\n",
        "ensure(\"torchviz\")\n",
        "ensure(\"torch_summary\", \"torch-summary\")\n",
        "ensure(\"seaborn\")\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torchview import draw_graph\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-eDK_jJLcMN"
      },
      "source": [
        "## 2.&nbsp;Hyperparameters\n",
        "Central config dictionary - change values here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "adS0YbA_ND4w"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'num_channels': 5,\n",
        "    'signal_length': 100000,\n",
        "    'input_window': 500,\n",
        "\n",
        "    # Horizon experiment\n",
        "    'horizons': [1, 2, 3, 5, 10],\n",
        "    'horizon': None,\n",
        "    \n",
        "    'avg_duration': 40,\n",
        "    'hidden_size': 64,\n",
        "    'batch_size': 64,\n",
        "    'num_epochs': 100,\n",
        "    'learning_rate': 0.001,\n",
        "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "}\n",
        "\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-6tgFbULffj"
      },
      "source": [
        "## 3.&nbsp;Plotting functions\n",
        "Contains functions that produce a plot, used later in code for data visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FDfMpyxPNLJq"
      },
      "outputs": [],
      "source": [
        "# 1. Shows all 5 channels & their states\n",
        "def plot_channels(channel_data, num_channels):\n",
        "    plt.figure(figsize=(14, 4))\n",
        "\n",
        "    offset = 1.5\n",
        "    yticks = []\n",
        "    yticklabels = []\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        rev_i = num_channels - 1 - i\n",
        "        channel_offset = rev_i * offset\n",
        "\n",
        "        # Plot actual channel signal\n",
        "        plt.plot(channel_data[i] + channel_offset, label=f\"Channel {i+1}\")\n",
        "\n",
        "        # Add horizontal lines at 0 and 1 for this channel\n",
        "        plt.hlines(y=channel_offset + 0, xmin=0, xmax=channel_data.shape[1]-1, colors='gray', linestyles='dashed', linewidth=0.8)\n",
        "        plt.hlines(y=channel_offset + 1, xmin=0, xmax=channel_data.shape[1]-1, colors='gray', linestyles='dashed', linewidth=0.8)\n",
        "\n",
        "        # Set middle point for tick\n",
        "        mid = channel_offset + 0.5\n",
        "        yticks.append(mid)\n",
        "        yticklabels.append(f\"Ch {i+1}\")\n",
        "\n",
        "    plt.title(\"Simulated Channel Occupancy\")\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"Occupancy\")\n",
        "    plt.yticks(yticks, yticklabels)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1.01, 1), borderaxespad=0)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 2. Shows heatmap sample from dataset, red = 0, green = 1\n",
        "def heatmap_dataset_sample_binary(X, y, sample_index=0, input_window=100, horizon=config['horizon']):\n",
        "    marker_position = input_window + horizon  # 110\n",
        "    # Select the sample\n",
        "    X_sample = X[sample_index]\n",
        "    y_sample = y[sample_index]\n",
        "\n",
        "    # Combine input and horizon along time axis\n",
        "    combined = np.concatenate([X_sample, y_sample], axis=0).T  # shape -> (5, 110)\n",
        "    plt.figure(figsize=(13, 8))\n",
        "\n",
        "    ax = sns.heatmap(\n",
        "        combined,\n",
        "        cmap=[\"#d9534f\", \"#5cb85c\"],\n",
        "        cbar=False,\n",
        "        linewidths=0.5,\n",
        "        linecolor=\"black\",\n",
        "        square=True\n",
        "    )\n",
        "    ax.set_xlabel(\"Time step\", fontsize=16)\n",
        "    ax.set_ylabel(\"Channel\", fontsize=16)\n",
        "\n",
        "    # Channels\n",
        "    ax.set_yticks(np.arange(combined.shape[0]) + 0.5)\n",
        "    ax.set_yticklabels([str(i+1) for i in range(combined.shape[0])], fontsize=6)\n",
        "\n",
        "    # Vertical line separating input and horizon\n",
        "    ax.axvline(input_window, color=\"black\", linewidth=4)\n",
        "    ax.text(input_window + 0.5, -0.5, \"←Horizon→\", fontsize=10, color=\"black\")\n",
        "\n",
        "    # Reduce x-tick clutter\n",
        "    ax.set_xticks(np.arange(0, combined.shape[1], 10))\n",
        "    ax.set_xticklabels(np.arange(0, combined.shape[1], 10), rotation=90)\n",
        "\n",
        "    # Ensure 110 is included as a tick and appears below the heatmap\n",
        "    xticks = list(np.arange(0, combined.shape[1], 10))\n",
        "    if marker_position not in xticks:\n",
        "        xticks.append(marker_position)\n",
        "    ax.set_xticks(xticks)\n",
        "    ax.set_xticklabels([str(t) for t in xticks], rotation=90)\n",
        "\n",
        "    # Thick vertical line at 110\n",
        "    ax.axvline(marker_position, color='black', linewidth=3)\n",
        "    plt.title(f\"Dataset Sample #{sample_index}\", fontsize=16, pad=20)\n",
        "\n",
        "    # Define legend entries\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='#d9534f', edgecolor='black', label='0'),\n",
        "        Patch(facecolor='#5cb85c', edgecolor='black', label='1')\n",
        "    ]\n",
        "    # Add legend to the axes\n",
        "    ax.legend(handles=legend_elements, title='Value', loc='upper right', bbox_to_anchor=(1.08, 1.2))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 3. Shows loss curve plotted over num. of epochs\n",
        "def plot_loss(loss_values, num_epochs):\n",
        "    min_loss_value = min(loss_values)\n",
        "    min_loss_epoch = loss_values.index(min_loss_value) + 1  # +1 because num_epochs start at 1\n",
        "    max_loss_value = max(loss_values)\n",
        "    last_loss_value = loss_values[num_epochs-1]\n",
        "    # Loss curve plot with best loss highlighted\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, num_epochs + 1), loss_values, marker='o', linestyle='-', label='Training Loss')\n",
        "\n",
        "    # Highlight the best loss\n",
        "    plt.plot(min_loss_epoch, min_loss_value, 'ro', label='Best Loss')\n",
        "    plt.annotate(f'Best: {min_loss_value:.4f}',\n",
        "                xy=(min_loss_epoch, min_loss_value),\n",
        "                xytext=(min_loss_epoch + 1, min_loss_value + 0.01),\n",
        "                arrowprops=dict(facecolor='red', arrowstyle='->'))\n",
        "\n",
        "    plt.title(\"Training Loss Over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 4. Compares real channel data with predicted channel data (model doesn't predict first 100 time steps)\n",
        "def plot_compare_channels(X_raw, pred_full, warmup=config['input_window']):\n",
        "\n",
        "    pred_plot = pred_full.copy()\n",
        "    pred_plot[:, :warmup] = np.nan  # model doesn't predict first 100 time steps\n",
        "\n",
        "    num_channels = X_raw.shape[0]\n",
        "    fig, axes = plt.subplots(num_channels, 1, figsize=(35, 2*num_channels), sharex=False)\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Solid line for actual\n",
        "        ax.plot(X_raw[i], label=f\"Channel {i+1} Actual\", linewidth=2)\n",
        "\n",
        "        # Dashed line for predicted\n",
        "        ax.plot(pred_plot[i], '--', label=f\"Channel {i+1} Predicted\", linewidth=2)\n",
        "\n",
        "        ax.set_ylabel(\"Occupancy\")\n",
        "        ax.set_ylim(-0.1, 1.1)\n",
        "        ax.set_yticks([0, 1])\n",
        "        ax.set_title(f\"Channel {i+1}\")\n",
        "\n",
        "        # Horizontal 0/1 guide lines\n",
        "        ax.hlines([0, 1], xmin=0, xmax=X_raw.shape[1]-1, colors='gray', linestyles='dashed', linewidth=0.8)\n",
        "\n",
        "        # Vertical guidelines\n",
        "        ax.set_xticks(np.arange(0, X_raw.shape[1]+1, 50))\n",
        "        ax.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "        ax.legend(loc='upper left', bbox_to_anchor=(1.01, 1), borderaxespad=0)\n",
        "        ax.margins(x=0.01)\n",
        "\n",
        "    axes[-1].set_xlabel(\"Time step\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 5. Shows heatmap of first 100 predictions the model makes\n",
        "def heatmap_correctness_100(X_raw, pred_full, warmup=100, steps=config['input_window']):\n",
        "\n",
        "    X_raw_np = X_raw[:, warmup:warmup + steps]\n",
        "    pred_full_np = pred_full[:, warmup:warmup + steps]\n",
        "\n",
        "    pred_bin = (pred_full_np >= 0.5).astype(int)\n",
        "\n",
        "    correct_matrix = (pred_bin == X_raw_np).astype(int)\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    ax = sns.heatmap(\n",
        "        correct_matrix,\n",
        "        cmap=[\"#d9534f\", \"#5cb85c\"],\n",
        "        cbar=False,\n",
        "        linewidths=0.5,\n",
        "        linecolor=\"black\",\n",
        "        square=True\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Time step\", fontsize=12)\n",
        "    ax.set_ylabel(\"Channel\", fontsize=12)\n",
        "\n",
        "    ax.set_yticks(np.arange(correct_matrix.shape[0]) + 0.5)\n",
        "    ax.set_yticklabels([str(i+1) for i in range(correct_matrix.shape[0])], rotation=90, fontsize=6)\n",
        "\n",
        "    x_ticks = list(np.arange(0, correct_matrix.shape[1], 20))\n",
        "    if correct_matrix.shape[1] not in x_ticks:\n",
        "        x_ticks.append(correct_matrix.shape[1])\n",
        "\n",
        "    ax.set_xticks(x_ticks)\n",
        "    ax.set_xticklabels([str(warmup + tick) for tick in x_ticks], rotation=90, fontsize=10)\n",
        "\n",
        "    plt.title(f\"Prediction Accuracy Heatmap ({warmup}–{warmup+steps} steps)\", fontsize=18, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVkPvKYpLuhe"
      },
      "source": [
        "## 4.&nbsp;Simulate channel data\n",
        "Simulates channels - Return array shape (num_channels, length) with 0/1 occupancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sinP_OV_NNok"
      },
      "outputs": [],
      "source": [
        "def simulate_multiple_channels(num_channels=config['num_channels'],\n",
        "                               signal_length=config['signal_length'],\n",
        "                               avg_duration=config['avg_duration']):\n",
        "    channels = []\n",
        "    for _ in range(num_channels):\n",
        "        channel = []\n",
        "        state = 0  # start with 0 or could randomize\n",
        "        while len(channel) < signal_length:\n",
        "            # Exponential duration for both states\n",
        "            duration = np.random.exponential(scale=avg_duration)\n",
        "            duration = int(max(1, round(duration)))\n",
        "            channel.extend([state] * duration)\n",
        "            state = 1 - state  # flip state\n",
        "        # truncate to exact signal length\n",
        "        channels.append(channel[:signal_length])\n",
        "\n",
        "    return np.array(channels)\n",
        "\n",
        "channel_data = simulate_multiple_channels()\n",
        "plot_channels(channel_data, config['num_channels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-D_fGKNL6Jz"
      },
      "source": [
        "## 5.&nbsp;Sequence creation & DataLoader\n",
        "Converts channel data into X y sliding windows & creates PyTorch DataLoader, creates X & y windows for every horizons value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OkuTMnZrObuF"
      },
      "outputs": [],
      "source": [
        "channel_data = simulate_multiple_channels().T  # (time_steps, num_channels)\n",
        "\n",
        "def create_sequence(channel_data, input_window, horizon):\n",
        "    \"\"\"\n",
        "    Creates non-overlapping sequences for a given horizon.\n",
        "    \n",
        "    Returns:\n",
        "        X: np.array of shape (num_samples, input_window, num_channels)\n",
        "        y: np.array of shape (num_samples, horizon, num_channels)\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    step = input_window + horizon  # non-overlapping\n",
        "    max_start = len(channel_data) - step + 1\n",
        "\n",
        "    for i in range(0, max_start, step):\n",
        "        X.append(channel_data[i : i + input_window])\n",
        "        y.append(channel_data[i + input_window : i + step])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# If config['horizons'] contains multiple horizons\n",
        "X_horizons = {}\n",
        "y_horizons = {}\n",
        "\n",
        "for horizon in config['horizons']:  # e.g., [1,2,3,5,10]\n",
        "    X, y = create_sequence(channel_data, input_window=config['input_window'], horizon=horizon)\n",
        "    print(f\"Horizon={horizon}: X.shape={X.shape}, y.shape={y.shape}\")\n",
        "    \n",
        "    # Store sequences in dictionaries\n",
        "    X_horizons[horizon] = X\n",
        "    y_horizons[horizon] = y\n",
        "\n",
        "# Example: create DataLoaders for a specific horizon (say 1)\n",
        "h = 1\n",
        "X_tensor = torch.tensor(X_horizons[h], dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_horizons[h], dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "S8h8Z9Rej028"
      },
      "outputs": [],
      "source": [
        "# Show heatmap sample from dataset\n",
        "#heatmap_dataset_sample_binary(X, y, sample_index=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXTUz7rOMFLL"
      },
      "source": [
        "## 6.&nbsp;LSTM model definition\n",
        "Defines the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Cjd_Lq2bPEwD"
      },
      "outputs": [],
      "source": [
        "class LSTM_model(nn.Module):\n",
        "    def __init__(self, num_channels, horizon, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.horizon = horizon  # number of future time steps to predict\n",
        "        self.num_channels = num_channels  # number of channels/features per time step\n",
        "\n",
        "        self.lstm = nn.LSTM(  # defines the LSTM layer:\n",
        "            input_size=num_channels,  # input_size = num_channels:\n",
        "                                     # dimensionality of each input vector per time step (e.g., 5 channels)\n",
        "            hidden_size=hidden_size,  # hidden_size = hidden_size:\n",
        "                                      # dimensionality of the LSTM hidden state\n",
        "            batch_first=True          # batch_first=True:\n",
        "                                      # input/output tensors are shaped as (batch, seq_len, features)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(  # defines the fully connected (Linear) layer:\n",
        "            hidden_size,      # maps from the final LSTM hidden state (hidden_size)\n",
        "            num_channels * horizon  # to horizon future steps for each channel\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, input_window, num_channels)\n",
        "        out, _ = self.lstm(x)      # out: (batch_size, input_window, hidden_size)\n",
        "        last_out = out[:, -1, :]   # take the last time step:\n",
        "                                   # last_out: (batch_size, hidden_size)\n",
        "\n",
        "        preds = self.fc(last_out)  # preds: (batch_size, num_channels * horizon)\n",
        "\n",
        "        preds = preds.view(\n",
        "            -1,\n",
        "            self.horizon,\n",
        "            self.num_channels\n",
        "        )                           # reshape to:\n",
        "                                    # (batch_size, horizon, num_channels)\n",
        "\n",
        "        return torch.sigmoid(preds)  # sigmoid to output probabilities in range [0,1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHSHqKGMMUZw"
      },
      "source": [
        "## 7.&nbsp;Training loop\n",
        "Training loop for models, trains model for each horizons value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dwFmQoKfPHRt"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store all trained models, best states, and loss histories\n",
        "trained_models = {}  # {horizon: model}\n",
        "best_states = {}     # {horizon: best_model_state}\n",
        "best_losses = {}     # {horizon: best_loss}\n",
        "loss_histories = {}  # {horizon: [loss per epoch]}\n",
        "\n",
        "# Generate channel data once\n",
        "channel_data = simulate_multiple_channels().T  # shape: (time_steps, num_channels)\n",
        "\n",
        "for h in config['horizons']:\n",
        "    print(f\"\\n=== Training model for horizon = {h} ===\")\n",
        "\n",
        "    # Create non-overlapping sequences for this horizon\n",
        "    X, y = create_sequence(\n",
        "        channel_data,\n",
        "        input_window=config['input_window'],\n",
        "        horizon=h\n",
        "    )\n",
        "    print(f\"X.shape = {X.shape}, y.shape = {y.shape}\")\n",
        "\n",
        "    # DataLoader\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = LSTM_model(\n",
        "        num_channels=config['num_channels'],\n",
        "        horizon=h,\n",
        "        hidden_size=config['hidden_size']\n",
        "    ).to(config['device'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Track best model\n",
        "    best_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    loss_values = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, config['num_epochs'] + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch = X_batch.to(config['device'])\n",
        "            y_batch = y_batch.to(config['device'])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataset)\n",
        "        loss_values.append(avg_loss)\n",
        "        print(f\"Epoch {epoch}/{config['num_epochs']} - Loss: {avg_loss:.4f}\", flush=True)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"Training complete for horizon {h}. Best loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Save everything for this horizon\n",
        "    trained_models[h] = model\n",
        "    best_states[h] = best_model_state\n",
        "    best_losses[h] = best_loss\n",
        "    loss_histories[h] = loss_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Lvr5sfSHW5oz"
      },
      "outputs": [],
      "source": [
        "def plot_training_loss(loss_values, horizon, figsize=(8, 5), offset=0.03):\n",
        "    \"\"\"\n",
        "    Plots the training loss over epochs for a single horizon,\n",
        "    marking the best (lowest) loss with a red dot and annotation.\n",
        "\n",
        "    Args:\n",
        "        loss_values: list of float\n",
        "            Training loss per epoch.\n",
        "        horizon: int\n",
        "            Horizon value for the model (used in the title).\n",
        "        figsize: tuple\n",
        "            Size of the figure (width, height).\n",
        "        offset: float\n",
        "            Vertical offset for the annotation in data units (y-axis units).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.plot(epochs, loss_values, marker='o', color='dodgerblue', label='Training Loss')\n",
        "\n",
        "    # Best loss\n",
        "    best_loss = min(loss_values)\n",
        "    best_epoch = loss_values.index(best_loss) + 1\n",
        "\n",
        "    # Mark best loss\n",
        "    plt.scatter(best_epoch, best_loss, color='red', zorder=5, label='Best Loss')\n",
        "    plt.annotate(f'{best_loss:.4f}',\n",
        "                 xy=(best_epoch, best_loss),\n",
        "                 xytext=(best_epoch, best_loss + offset),\n",
        "                 ha='center',\n",
        "                 color='red')\n",
        "\n",
        "    # Labels and styling\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training Loss per Epoch - Horizon {horizon}')\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for h in config['horizons']:\n",
        "    plot_training_loss(loss_histories[h], horizon=h)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AvRdlELMbBy"
      },
      "source": [
        "## 8.&nbsp;Evaluate models\n",
        "Evaluates the trained models on 2 datasets:\n",
        "1. Datasets generated with a random `avg_duration` value\n",
        "2. Datasets generated with the same `avg_duration` value that the models were trained on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "A5YPJSN1ZMOZ"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Multi-Horizon Evaluation Cell\n",
        "# -----------------------------\n",
        "\n",
        "def generate_test_data(avg_duration=None, horizon=None):\n",
        "    \"\"\"\n",
        "    Generates test sequences for evaluation.\n",
        "    \n",
        "    Args:\n",
        "        avg_duration: int or None\n",
        "            - If None, generate random avg_duration between 1 and 100.\n",
        "            - If int, use this value.\n",
        "        horizon: int, required\n",
        "            - Number of future steps to predict (sequence length for y)\n",
        "    \n",
        "    Returns:\n",
        "        test_X_tensor: torch.Tensor (num_samples, input_window, num_channels)\n",
        "        test_y_tensor: torch.Tensor (num_samples, horizon, num_channels)\n",
        "        X_raw_T: numpy array (time_steps, num_channels)\n",
        "        avg_duration_used: int\n",
        "    \"\"\"\n",
        "    if avg_duration is None:\n",
        "        avg_duration_used = random.randint(1, 100)\n",
        "    else:\n",
        "        avg_duration_used = avg_duration\n",
        "\n",
        "    # Generate raw channel data\n",
        "    X_raw = simulate_multiple_channels(\n",
        "        num_channels=config['num_channels'],\n",
        "        signal_length=config['signal_length'],\n",
        "        avg_duration=avg_duration_used\n",
        "    )\n",
        "    X_raw_T = X_raw.T\n",
        "\n",
        "    # Create sequences\n",
        "    test_X, test_y = create_sequence(\n",
        "        X_raw_T,\n",
        "        input_window=config['input_window'],\n",
        "        horizon=horizon\n",
        "    )\n",
        "\n",
        "    # Convert to tensors\n",
        "    test_X_tensor = torch.tensor(test_X, dtype=torch.float32).to(config['device'])\n",
        "    test_y_tensor = torch.tensor(test_y, dtype=torch.float32).to(config['device'])\n",
        "\n",
        "    return test_X_tensor, test_y_tensor, X_raw_T, avg_duration_used\n",
        "\n",
        "\n",
        "def reconstruct_full_prediction(preds_binary, X_raw_T, horizon):\n",
        "    \"\"\"\n",
        "    Reconstruct full-length prediction from non-overlapping windows.\n",
        "\n",
        "    Args:\n",
        "        preds_binary: torch.Tensor (num_samples, horizon, num_channels)\n",
        "        X_raw_T: numpy array (time_steps, num_channels)\n",
        "        horizon: int\n",
        "\n",
        "    Returns:\n",
        "        pred_full: numpy array (num_channels, signal_length)\n",
        "        X_raw: numpy array (num_channels, signal_length)\n",
        "    \"\"\"\n",
        "    preds_np = preds_binary.cpu().numpy()\n",
        "    num_samples, horizon, num_channels = preds_np.shape\n",
        "\n",
        "    X_raw = X_raw_T.T\n",
        "    signal_length = X_raw.shape[1]\n",
        "\n",
        "    pred_full = np.zeros((num_channels, signal_length))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        start = i\n",
        "        end = min(i + horizon, signal_length)\n",
        "        pred_full[:, start:end] = preds_np[i, :end-start, :].T\n",
        "\n",
        "    return pred_full, X_raw\n",
        "\n",
        "\n",
        "def evaluate_model(model, best_model_state, horizon, num_tests=20, test_type=\"random\"):\n",
        "    \"\"\"\n",
        "    Evaluate a single LSTM model on either random or training-distribution data.\n",
        "\n",
        "    Args:\n",
        "        model: trained LSTM model\n",
        "        best_model_state: model state dict\n",
        "        horizon: int, prediction horizon for this model\n",
        "        num_tests: number of test datasets to evaluate\n",
        "        test_type: \"random\" or \"train\"\n",
        "\n",
        "    Returns:\n",
        "        rows: list of strings for printing\n",
        "        saved_results: list of tuples with\n",
        "                       (pred_full, X_raw, preds_binary, avg_duration_used, accuracy)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    saved_results = []\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.to(config['device'])\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\nEvaluating horizon = {horizon} ({test_type} data)\")\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        # Determine avg_duration\n",
        "        if test_type == \"train\":\n",
        "            avg_duration_arg = config['avg_duration']\n",
        "        elif test_type == \"random\":\n",
        "            avg_duration_arg = None\n",
        "        else:\n",
        "            raise ValueError(\"test_type must be 'train' or 'random'\")\n",
        "\n",
        "        # Generate test data\n",
        "        test_X_tensor, test_y_tensor, X_raw_T, avg_duration_used = generate_test_data(\n",
        "            avg_duration=avg_duration_arg,\n",
        "            horizon=horizon\n",
        "        )\n",
        "\n",
        "        # Model prediction\n",
        "        with torch.no_grad():\n",
        "            preds = model(test_X_tensor)\n",
        "\n",
        "        # Binary prediction\n",
        "        preds_binary = (preds >= 0.5).int()\n",
        "\n",
        "        # Accuracy\n",
        "        correct = (preds_binary == test_y_tensor.int()).sum().item()\n",
        "        total = torch.numel(test_y_tensor)\n",
        "        acc_percent = (correct / total) * 100\n",
        "\n",
        "        # Reconstruct full-length prediction\n",
        "        pred_full, X_raw = reconstruct_full_prediction(preds_binary, X_raw_T, horizon=horizon)\n",
        "\n",
        "        # Save results\n",
        "        saved_results.append((pred_full, X_raw, preds_binary.cpu().numpy(), avg_duration_used, acc_percent))\n",
        "        rows.append(f\"{i+1:02d}\\t{avg_duration_used}\\t{acc_percent:.2f}\")\n",
        "\n",
        "    # Print table\n",
        "    print(\"\\nTest\\tavg_duration\\tAccuracy (%)\")\n",
        "    for r in rows:\n",
        "        print(r)\n",
        "\n",
        "    # Average accuracy\n",
        "    avg_acc = sum([res[4] for res in saved_results]) / len(saved_results)\n",
        "    print(f\"Average accuracy over {num_tests} tests: {avg_acc:.2f}%\\n\")\n",
        "\n",
        "    return rows, saved_results\n",
        "\n",
        "\n",
        "def evaluate_all_models(trained_models, best_states, num_tests=20):\n",
        "    \"\"\"\n",
        "    Evaluate all trained models for all horizons.\n",
        "\n",
        "    Args:\n",
        "        trained_models: dict {horizon: model}\n",
        "        best_states: dict {horizon: best_model_state}\n",
        "        num_tests: number of test datasets per model\n",
        "\n",
        "    Returns:\n",
        "        all_results: dict {horizon: {\"random\": (rows, saved_results),\n",
        "                                      \"train\":  (rows, saved_results)}}\n",
        "    \"\"\"\n",
        "    all_results = {}\n",
        "\n",
        "    for h, model in trained_models.items():\n",
        "        all_results[h] = {}\n",
        "\n",
        "        # Random test\n",
        "        rows_r, res_r = evaluate_model(model, best_states[h], horizon=h, num_tests=num_tests, test_type=\"random\")\n",
        "        all_results[h][\"random\"] = (rows_r, res_r)\n",
        "\n",
        "        # Training distribution test\n",
        "        rows_t, res_t = evaluate_model(model, best_states[h], horizon=h, num_tests=num_tests, test_type=\"train\")\n",
        "        all_results[h][\"train\"] = (rows_t, res_t)\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume you have trained_models = {h: model} and best_states = {h: best_model_state} for all horizons\n",
        "all_results = evaluate_all_models(trained_models, best_states, num_tests=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Visualization for multi-horizon results\n",
        "# -------------------------------\n",
        "\n",
        "def plot_horizon_accuracies(all_results):\n",
        "    horizons = sorted(all_results.keys())\n",
        "    avg_acc_random = []\n",
        "    avg_acc_train = []\n",
        "\n",
        "    # Compute average accuracy per horizon for both test types (for bar plot)\n",
        "    for h in horizons:\n",
        "        _, res_random = all_results[h][\"random\"]\n",
        "        _, res_train = all_results[h][\"train\"]\n",
        "\n",
        "        avg_acc_random.append(np.mean([r[4] for r in res_random]))\n",
        "        avg_acc_train.append(np.mean([r[4] for r in res_train]))\n",
        "\n",
        "    # --- Bar Plot ---\n",
        "    x = np.arange(len(horizons))  # positions\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.bar(x - width/2, avg_acc_random, width, label='Random Test', color='skyblue')\n",
        "    ax.bar(x + width/2, avg_acc_train, width, label='Training Distribution Test', color='salmon')\n",
        "\n",
        "    ax.set_xlabel('Horizon')\n",
        "    ax.set_ylabel('Average Accuracy (%)')\n",
        "    ax.set_title('Average Accuracy per Horizon')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(horizons)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # --- Line Plot per Horizon ---\n",
        "    for h in horizons:\n",
        "        _, res_random = all_results[h][\"random\"]\n",
        "        _, res_train = all_results[h][\"train\"]\n",
        "\n",
        "        random_accs = [r[4] for r in res_random]\n",
        "        train_accs = [r[4] for r in res_train]\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(range(1, len(random_accs)+1), random_accs, marker='o', label='Random Test', color='skyblue')\n",
        "        plt.plot(range(1, len(train_accs)+1), train_accs, marker='x', label='Training Distribution Test', color='salmon')\n",
        "\n",
        "        plt.xlabel('Test Dataset Index')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.title(f'Horizon {h} - Accuracy per Test Dataset')\n",
        "        plt.ylim(0, 100)\n",
        "        plt.xticks(range(1, len(random_accs)+1))\n",
        "        plt.legend()\n",
        "        plt.grid(linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Call the plotting function\n",
        "# -------------------------------\n",
        "plot_horizon_accuracies(all_results)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
